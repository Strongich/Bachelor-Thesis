2024-06-05 12:13:23,879 - root - INFO - Running on device=cuda
2024-06-05 12:13:24,768 - root - INFO - N_EPOCH: 20
2024-06-05 12:13:24,768 - root - INFO - LEARNING_RATE: 0.0005
2024-06-05 12:13:24,769 - root - INFO - HIDDEN_LAYER: 20
2024-06-05 12:13:24,769 - root - INFO - HIDDEN_WIDTH: 64
2024-06-05 12:13:24,769 - root - INFO - EXPERIMENT_NAME: exp_call_13_real
2024-06-05 12:13:24,769 - root - INFO - Model type: AmericanPut_gated3
2024-06-05 12:13:27,839 - root - INFO - Initial MSE train: 49.358, Initial MSE val: 36.1361
2024-06-05 12:13:29,686 - root - INFO - Total number of learnable parameters: 83725
2024-06-05 12:13:34,779 - root - INFO - Optimizer: SGD
2024-06-05 12:13:37,279 - root - INFO - Epoch 1, train loss: 40.9945, val loss: 27.6539
2024-06-05 12:13:39,848 - root - INFO - Epoch 2, train loss: 39.6288, val loss: 26.4929
2024-06-05 12:13:42,162 - root - INFO - Epoch 3, train loss: 39.4782, val loss: 27.2126
2024-06-05 12:13:44,499 - root - INFO - Epoch 4, train loss: 39.1418, val loss: 26.7336
2024-06-05 12:13:46,848 - root - INFO - Epoch 5, train loss: 38.8460, val loss: 26.0720
2024-06-05 12:13:49,151 - root - INFO - Epoch 6, train loss: 38.8080, val loss: 27.3162
2024-06-05 12:13:51,542 - root - INFO - Epoch 7, train loss: 38.6439, val loss: 26.4545
2024-06-05 12:13:53,943 - root - INFO - Epoch 8, train loss: 38.5165, val loss: 26.7733
2024-06-05 12:13:56,344 - root - INFO - Epoch 9, train loss: 38.4783, val loss: 26.3179
2024-06-05 12:13:58,631 - root - INFO - Epoch 10, train loss: 38.5116, val loss: 25.6962
2024-06-05 12:14:00,947 - root - INFO - Epoch 11, train loss: 38.1157, val loss: 26.0866
2024-06-05 12:14:03,281 - root - INFO - Epoch 12, train loss: 37.9261, val loss: 28.3419
2024-06-05 12:14:05,620 - root - INFO - Epoch 13, train loss: 37.8954, val loss: 26.0146
2024-06-05 12:14:07,935 - root - INFO - Epoch 14, train loss: 37.7616, val loss: 27.2330
2024-06-05 12:14:10,282 - root - INFO - Epoch 15, train loss: 37.7448, val loss: 25.8491
2024-06-05 12:14:12,833 - root - INFO - Epoch 16, train loss: 37.4749, val loss: 26.4283
2024-06-05 12:14:15,309 - root - INFO - Epoch 17, train loss: 37.4351, val loss: 26.2797
2024-06-05 12:14:17,641 - root - INFO - Epoch 18, train loss: 37.3298, val loss: 26.6910
2024-06-05 12:14:19,979 - root - INFO - Epoch 19, train loss: 37.2368, val loss: 26.7073
2024-06-05 12:14:22,407 - root - INFO - Epoch 20, train loss: 37.1723, val loss: 26.8327
2024-06-05 12:14:22,407 - root - INFO - Optimizer: RMSprop
2024-06-05 12:14:24,942 - root - INFO - Epoch 1, train loss: 39.5231, val loss: 27.6490
2024-06-05 12:14:27,369 - root - INFO - Epoch 2, train loss: 38.7270, val loss: 27.3024
2024-06-05 12:14:29,814 - root - INFO - Epoch 3, train loss: 38.3076, val loss: 26.4351
2024-06-05 12:14:32,235 - root - INFO - Epoch 4, train loss: 38.4260, val loss: 27.4912
2024-06-05 12:14:34,737 - root - INFO - Epoch 5, train loss: 38.3486, val loss: 26.0889
2024-06-05 12:14:37,117 - root - INFO - Epoch 6, train loss: 38.2947, val loss: 28.4814
2024-06-05 12:14:39,517 - root - INFO - Epoch 7, train loss: 38.1104, val loss: 26.7905
2024-06-05 12:14:41,935 - root - INFO - Epoch 8, train loss: 38.0846, val loss: 26.2264
2024-06-05 12:14:44,385 - root - INFO - Epoch 9, train loss: 37.8371, val loss: 26.2352
2024-06-05 12:14:46,755 - root - INFO - Epoch 10, train loss: 37.5980, val loss: 26.1377
2024-06-05 12:14:49,124 - root - INFO - Epoch 11, train loss: 37.6015, val loss: 29.6447
2024-06-05 12:14:51,667 - root - INFO - Epoch 12, train loss: 37.2286, val loss: 27.6139
2024-06-05 12:14:54,148 - root - INFO - Epoch 13, train loss: 37.5105, val loss: 27.4783
2024-06-05 12:14:56,683 - root - INFO - Epoch 14, train loss: 37.4022, val loss: 27.5608
2024-06-05 12:14:59,058 - root - INFO - Epoch 15, train loss: 37.1002, val loss: 27.2746
2024-06-05 12:15:01,464 - root - INFO - Epoch 16, train loss: 37.0338, val loss: 27.6989
2024-06-05 12:15:03,879 - root - INFO - Epoch 17, train loss: 36.9001, val loss: 27.3894
2024-06-05 12:15:06,306 - root - INFO - Epoch 18, train loss: 36.6834, val loss: 27.3369
2024-06-05 12:15:08,695 - root - INFO - Epoch 19, train loss: 36.6583, val loss: 27.6253
2024-06-05 12:15:11,318 - root - INFO - Epoch 20, train loss: 36.5772, val loss: 27.6178
2024-06-05 12:15:11,319 - root - INFO - Optimizer: Adam
2024-06-05 12:15:14,052 - root - INFO - Epoch 1, train loss: 37.9381, val loss: 27.1542
2024-06-05 12:15:16,615 - root - INFO - Epoch 2, train loss: 38.0282, val loss: 27.4187
2024-06-05 12:15:19,172 - root - INFO - Epoch 3, train loss: 37.7062, val loss: 26.3620
2024-06-05 12:15:21,933 - root - INFO - Epoch 4, train loss: 37.7498, val loss: 26.5560
2024-06-05 12:15:24,503 - root - INFO - Epoch 5, train loss: 37.8290, val loss: 26.3370
2024-06-05 12:15:27,233 - root - INFO - Epoch 6, train loss: 37.8584, val loss: 26.8796
2024-06-05 12:15:29,786 - root - INFO - Epoch 7, train loss: 37.6203, val loss: 27.8238
2024-06-05 12:15:32,374 - root - INFO - Epoch 8, train loss: 37.5760, val loss: 26.1038
2024-06-05 12:15:34,735 - root - INFO - Epoch 9, train loss: 37.8387, val loss: 27.5304
2024-06-05 12:15:37,182 - root - INFO - Epoch 10, train loss: 37.4957, val loss: 26.5323
2024-06-05 12:15:39,594 - root - INFO - Epoch 11, train loss: 37.3981, val loss: 29.3255
2024-06-05 12:15:41,960 - root - INFO - Epoch 12, train loss: 37.1992, val loss: 26.8633
2024-06-05 12:15:44,397 - root - INFO - Epoch 13, train loss: 37.0950, val loss: 28.0807
2024-06-05 12:15:46,727 - root - INFO - Epoch 14, train loss: 37.1785, val loss: 27.6377
2024-06-05 12:15:49,087 - root - INFO - Epoch 15, train loss: 37.0267, val loss: 27.7801
2024-06-05 12:15:51,386 - root - INFO - Epoch 16, train loss: 36.7912, val loss: 28.3463
2024-06-05 12:15:53,808 - root - INFO - Epoch 17, train loss: 36.6454, val loss: 28.1240
2024-06-05 12:15:56,105 - root - INFO - Epoch 18, train loss: 36.5532, val loss: 28.2203
2024-06-05 12:15:58,488 - root - INFO - Epoch 19, train loss: 36.5030, val loss: 28.0781
2024-06-05 12:16:00,737 - root - INFO - Epoch 20, train loss: 36.4651, val loss: 28.1590
2024-06-05 12:16:00,737 - root - INFO - Optimizer: Adamax
2024-06-05 12:16:03,830 - root - INFO - Epoch 1, train loss: 37.0420, val loss: 27.5408
2024-06-05 12:16:07,313 - root - INFO - Epoch 2, train loss: 36.9113, val loss: 28.7879
2024-06-05 12:16:11,018 - root - INFO - Epoch 3, train loss: 36.7399, val loss: 29.2278
2024-06-05 12:16:14,501 - root - INFO - Epoch 4, train loss: 36.7186, val loss: 28.6121
2024-06-05 12:16:17,886 - root - INFO - Epoch 5, train loss: 36.5995, val loss: 28.5484
2024-06-05 12:16:21,358 - root - INFO - Epoch 6, train loss: 36.6401, val loss: 30.5425
2024-06-05 12:16:24,941 - root - INFO - Epoch 7, train loss: 36.5734, val loss: 31.2936
2024-06-05 12:16:28,343 - root - INFO - Epoch 8, train loss: 36.5168, val loss: 30.1537
2024-06-05 12:16:31,765 - root - INFO - Epoch 9, train loss: 36.5148, val loss: 29.5917
2024-06-05 12:16:35,316 - root - INFO - Epoch 10, train loss: 36.2972, val loss: 30.2821
2024-06-05 12:16:38,794 - root - INFO - Epoch 11, train loss: 36.3654, val loss: 30.3767
2024-06-05 12:16:42,240 - root - INFO - Epoch 12, train loss: 36.3668, val loss: 30.6113
2024-06-05 12:16:45,708 - root - INFO - Epoch 13, train loss: 36.2113, val loss: 31.1067
2024-06-05 12:16:49,139 - root - INFO - Epoch 14, train loss: 36.1129, val loss: 30.7002
2024-06-05 12:16:52,678 - root - INFO - Epoch 15, train loss: 36.0321, val loss: 31.5645
2024-06-05 12:16:56,202 - root - INFO - Epoch 16, train loss: 36.0459, val loss: 31.5766
2024-06-05 12:16:59,676 - root - INFO - Epoch 17, train loss: 35.9383, val loss: 31.4259
2024-06-05 12:17:03,128 - root - INFO - Epoch 18, train loss: 35.9134, val loss: 31.4195
2024-06-05 12:17:06,609 - root - INFO - Epoch 19, train loss: 35.8979, val loss: 31.5167
2024-06-05 12:17:10,103 - root - INFO - Epoch 20, train loss: 35.8768, val loss: 31.5188
2024-06-05 12:22:28,052 - root - INFO - Running on device=cuda
2024-06-05 12:22:28,723 - root - INFO - N_EPOCH: 50
2024-06-05 12:22:28,724 - root - INFO - LEARNING_RATE: 0.00075178
2024-06-05 12:22:28,724 - root - INFO - HIDDEN_LAYER: 24
2024-06-05 12:22:28,724 - root - INFO - HIDDEN_WIDTH: 64
2024-06-05 12:22:28,724 - root - INFO - EXPERIMENT_NAME: exp_call_13_gen
2024-06-05 12:22:28,725 - root - INFO - Model type: AmericanPut_gated3
2024-06-05 12:22:50,634 - root - INFO - Initial MSE train: 3309.9748, Initial MSE val: 3447.6063
2024-06-05 12:22:53,902 - root - INFO - Total number of learnable parameters: 100365
2024-06-05 12:23:00,610 - root - INFO - Optimizer: SGD
2024-06-05 12:23:02,041 - root - INFO - Epoch 1, train loss: 33978.6318, val loss: 7541.3600
2024-06-05 12:23:03,548 - root - INFO - Epoch 2, train loss: 4670.4073, val loss: 3539.7367
2024-06-05 12:23:05,031 - root - INFO - Epoch 3, train loss: 2965.4831, val loss: 2683.6173
2024-06-05 12:23:06,539 - root - INFO - Epoch 4, train loss: 2501.0461, val loss: 2517.2730
2024-06-05 12:23:08,188 - root - INFO - Epoch 5, train loss: 2284.3642, val loss: 2363.4223
2024-06-05 12:23:09,720 - root - INFO - Epoch 6, train loss: 2269.9208, val loss: 2537.1974
2024-06-05 12:23:11,392 - root - INFO - Epoch 7, train loss: 2298.5690, val loss: 2454.1338
2024-06-05 12:23:13,010 - root - INFO - Epoch 8, train loss: 2280.7166, val loss: 2494.4259
2024-06-05 12:23:14,564 - root - INFO - Epoch 9, train loss: 2226.6197, val loss: 2452.9201
2024-06-05 12:23:16,219 - root - INFO - Epoch 10, train loss: 2247.2231, val loss: 2308.4700
2024-06-05 12:23:17,915 - root - INFO - Epoch 11, train loss: 2191.3770, val loss: 2303.1888
2024-06-05 12:23:19,493 - root - INFO - Epoch 12, train loss: 2151.0691, val loss: 2283.1307
2024-06-05 12:23:21,066 - root - INFO - Epoch 13, train loss: 2171.4402, val loss: 2185.3479
2024-06-05 12:23:22,703 - root - INFO - Epoch 14, train loss: 2223.1101, val loss: 2253.1397
2024-06-05 12:23:24,272 - root - INFO - Epoch 15, train loss: 2209.0477, val loss: 2336.7474
2024-06-05 12:23:25,917 - root - INFO - Epoch 16, train loss: 2215.1647, val loss: 2416.7592
2024-06-05 12:23:27,532 - root - INFO - Epoch 17, train loss: 2215.9643, val loss: 2192.7351
2024-06-05 12:23:29,143 - root - INFO - Epoch 18, train loss: 2203.4856, val loss: 2463.2537
2024-06-05 12:23:30,711 - root - INFO - Epoch 19, train loss: 2176.7527, val loss: 2303.2514
2024-06-05 12:23:32,399 - root - INFO - Epoch 20, train loss: 2188.1820, val loss: 2382.6728
2024-06-05 12:23:34,012 - root - INFO - Epoch 21, train loss: 2244.4789, val loss: 2347.2126
2024-06-05 12:23:35,644 - root - INFO - Epoch 22, train loss: 2246.3789, val loss: 2394.9221
2024-06-05 12:23:37,302 - root - INFO - Epoch 23, train loss: 2215.3490, val loss: 2352.0276
2024-06-05 12:23:38,890 - root - INFO - Epoch 24, train loss: 2231.9973, val loss: 2465.6559
2024-06-05 12:23:40,530 - root - INFO - Epoch 25, train loss: 2293.7304, val loss: 2367.0031
2024-06-05 12:23:42,100 - root - INFO - Epoch 26, train loss: 2193.7217, val loss: 2465.7302
2024-06-05 12:23:43,714 - root - INFO - Epoch 27, train loss: 2200.3842, val loss: 2325.7695
2024-06-05 12:23:45,256 - root - INFO - Epoch 28, train loss: 2176.0909, val loss: 2335.6858
2024-06-05 12:23:46,762 - root - INFO - Epoch 29, train loss: 2146.6544, val loss: 2273.3208
2024-06-05 12:23:48,367 - root - INFO - Epoch 30, train loss: 2159.2085, val loss: 2313.8913
2024-06-05 12:23:49,916 - root - INFO - Epoch 31, train loss: 2199.7221, val loss: 2320.1936
2024-06-05 12:23:51,493 - root - INFO - Epoch 32, train loss: 2207.7969, val loss: 2380.0764
2024-06-05 12:23:53,151 - root - INFO - Epoch 33, train loss: 2232.7749, val loss: 2542.6065
2024-06-05 12:23:54,779 - root - INFO - Epoch 34, train loss: 2284.1873, val loss: 2467.6335
2024-06-05 12:23:56,351 - root - INFO - Epoch 35, train loss: 2256.3098, val loss: 2466.2555
2024-06-05 12:23:57,950 - root - INFO - Epoch 36, train loss: 2228.7364, val loss: 2474.7901
2024-06-05 12:23:59,489 - root - INFO - Epoch 37, train loss: 2255.8094, val loss: 2434.3029
2024-06-05 12:24:01,049 - root - INFO - Epoch 38, train loss: 2247.7372, val loss: 2505.5405
2024-06-05 12:24:02,596 - root - INFO - Epoch 39, train loss: 2252.8323, val loss: 2482.3473
2024-06-05 12:24:04,154 - root - INFO - Epoch 40, train loss: 2250.1073, val loss: 2453.5441
2024-06-05 12:24:05,708 - root - INFO - Epoch 41, train loss: 2249.0037, val loss: 2396.5093
2024-06-05 12:24:07,276 - root - INFO - Epoch 42, train loss: 2221.1514, val loss: 2432.5548
2024-06-05 12:24:08,828 - root - INFO - Epoch 43, train loss: 2218.3549, val loss: 2427.5216
2024-06-05 12:24:10,412 - root - INFO - Epoch 44, train loss: 2223.1816, val loss: 2397.5437
2024-06-05 12:24:11,999 - root - INFO - Epoch 45, train loss: 2213.6623, val loss: 2377.3002
2024-06-05 12:24:13,551 - root - INFO - Epoch 46, train loss: 2204.3014, val loss: 2343.1897
2024-06-05 12:24:15,131 - root - INFO - Epoch 47, train loss: 2183.3742, val loss: 2365.8129
2024-06-05 12:24:16,730 - root - INFO - Epoch 48, train loss: 2189.8528, val loss: 2332.2966
2024-06-05 12:24:18,266 - root - INFO - Epoch 49, train loss: 2181.2418, val loss: 2338.6049
2024-06-05 12:24:19,802 - root - INFO - Epoch 50, train loss: 2177.8343, val loss: 2343.8155
2024-06-05 12:24:19,802 - root - INFO - Optimizer: RMSprop
2024-06-05 12:24:21,447 - root - INFO - Epoch 1, train loss: 2185.5655, val loss: 2329.5183
2024-06-05 12:24:23,131 - root - INFO - Epoch 2, train loss: 2165.6447, val loss: 2314.6339
2024-06-05 12:24:24,778 - root - INFO - Epoch 3, train loss: 2154.6373, val loss: 2321.2948
2024-06-05 12:24:26,477 - root - INFO - Epoch 4, train loss: 2148.8110, val loss: 2319.0806
2024-06-05 12:24:28,091 - root - INFO - Epoch 5, train loss: 2151.5936, val loss: 2324.3204
2024-06-05 12:24:29,696 - root - INFO - Epoch 6, train loss: 2138.4037, val loss: 2311.8177
2024-06-05 12:24:31,298 - root - INFO - Epoch 7, train loss: 2133.8231, val loss: 2327.7028
2024-06-05 12:24:32,889 - root - INFO - Epoch 8, train loss: 2132.6636, val loss: 2323.0651
2024-06-05 12:24:34,535 - root - INFO - Epoch 9, train loss: 2128.2706, val loss: 2323.6883
2024-06-05 12:24:36,206 - root - INFO - Epoch 10, train loss: 2127.7959, val loss: 2317.7407
2024-06-05 12:24:37,863 - root - INFO - Epoch 11, train loss: 2121.0123, val loss: 2316.7002
2024-06-05 12:24:39,532 - root - INFO - Epoch 12, train loss: 2120.0514, val loss: 2325.5922
2024-06-05 12:24:41,122 - root - INFO - Epoch 13, train loss: 2116.0591, val loss: 2311.2428
2024-06-05 12:24:42,797 - root - INFO - Epoch 14, train loss: 2115.0338, val loss: 2313.0310
2024-06-05 12:24:44,404 - root - INFO - Epoch 15, train loss: 2114.4213, val loss: 2316.6886
2024-06-05 12:24:46,052 - root - INFO - Epoch 16, train loss: 2105.8583, val loss: 2313.7722
2024-06-05 12:24:47,659 - root - INFO - Epoch 17, train loss: 2101.6593, val loss: 2319.1779
2024-06-05 12:24:49,263 - root - INFO - Epoch 18, train loss: 2096.7436, val loss: 2309.2723
2024-06-05 12:24:50,951 - root - INFO - Epoch 19, train loss: 2094.9283, val loss: 2311.2385
2024-06-05 12:24:52,700 - root - INFO - Epoch 20, train loss: 2089.4951, val loss: 2314.2388
2024-06-05 12:24:54,340 - root - INFO - Epoch 21, train loss: 2092.5310, val loss: 2310.8167
2024-06-05 12:24:56,017 - root - INFO - Epoch 22, train loss: 2090.5214, val loss: 2318.4568
2024-06-05 12:24:57,671 - root - INFO - Epoch 23, train loss: 2086.2841, val loss: 2308.5363
2024-06-05 12:24:59,352 - root - INFO - Epoch 24, train loss: 2081.4438, val loss: 2309.1041
2024-06-05 12:25:00,948 - root - INFO - Epoch 25, train loss: 2080.6326, val loss: 2318.3479
2024-06-05 12:25:02,527 - root - INFO - Epoch 26, train loss: 2076.2120, val loss: 2315.3213
2024-06-05 12:25:04,142 - root - INFO - Epoch 27, train loss: 2077.9227, val loss: 2316.5087
2024-06-05 12:25:05,812 - root - INFO - Epoch 28, train loss: 2076.6934, val loss: 2312.8404
2024-06-05 12:25:07,429 - root - INFO - Epoch 29, train loss: 2074.0470, val loss: 2311.6992
2024-06-05 12:25:09,071 - root - INFO - Epoch 30, train loss: 2072.0819, val loss: 2320.2955
2024-06-05 12:25:10,747 - root - INFO - Epoch 31, train loss: 2074.3229, val loss: 2317.1220
2024-06-05 12:25:12,329 - root - INFO - Epoch 32, train loss: 2070.6291, val loss: 2312.2657
2024-06-05 12:25:13,950 - root - INFO - Epoch 33, train loss: 2067.5084, val loss: 2309.6699
2024-06-05 12:25:15,620 - root - INFO - Epoch 34, train loss: 2064.0270, val loss: 2313.4968
2024-06-05 12:25:17,221 - root - INFO - Epoch 35, train loss: 2063.0871, val loss: 2312.9673
2024-06-05 12:25:18,838 - root - INFO - Epoch 36, train loss: 2061.7003, val loss: 2309.6899
2024-06-05 12:25:20,421 - root - INFO - Epoch 37, train loss: 2061.7617, val loss: 2311.6924
2024-06-05 12:25:22,130 - root - INFO - Epoch 38, train loss: 2061.8683, val loss: 2304.0207
2024-06-05 12:25:23,780 - root - INFO - Epoch 39, train loss: 2060.1972, val loss: 2308.7463
2024-06-05 12:25:25,444 - root - INFO - Epoch 40, train loss: 2058.1004, val loss: 2305.1755
2024-06-05 12:25:27,079 - root - INFO - Epoch 41, train loss: 2057.9392, val loss: 2305.6634
2024-06-05 12:25:28,660 - root - INFO - Epoch 42, train loss: 2058.1633, val loss: 2300.5522
2024-06-05 12:25:30,351 - root - INFO - Epoch 43, train loss: 2058.1942, val loss: 2301.7455
2024-06-05 12:25:31,931 - root - INFO - Epoch 44, train loss: 2056.4706, val loss: 2300.3898
2024-06-05 12:25:33,535 - root - INFO - Epoch 45, train loss: 2057.0569, val loss: 2301.6791
2024-06-05 12:25:35,231 - root - INFO - Epoch 46, train loss: 2054.7623, val loss: 2302.1789
2024-06-05 12:25:36,906 - root - INFO - Epoch 47, train loss: 2055.8218, val loss: 2302.4620
2024-06-05 12:25:38,697 - root - INFO - Epoch 48, train loss: 2055.1079, val loss: 2301.7325
2024-06-05 12:25:40,421 - root - INFO - Epoch 49, train loss: 2054.3263, val loss: 2302.6173
2024-06-05 12:25:42,058 - root - INFO - Epoch 50, train loss: 2054.7577, val loss: 2302.6440
2024-06-05 12:25:42,058 - root - INFO - Optimizer: Adam
2024-06-05 12:25:43,849 - root - INFO - Epoch 1, train loss: 2075.0707, val loss: 2314.2549
2024-06-05 12:25:45,698 - root - INFO - Epoch 2, train loss: 2093.9150, val loss: 2292.7338
2024-06-05 12:25:47,419 - root - INFO - Epoch 3, train loss: 2092.5388, val loss: 2328.6843
2024-06-05 12:25:49,066 - root - INFO - Epoch 4, train loss: 2087.1883, val loss: 2314.1110
2024-06-05 12:25:50,768 - root - INFO - Epoch 5, train loss: 2086.2329, val loss: 2322.4527
2024-06-05 12:25:52,598 - root - INFO - Epoch 6, train loss: 2083.1481, val loss: 2316.5881
2024-06-05 12:25:54,355 - root - INFO - Epoch 7, train loss: 2086.3921, val loss: 2319.1414
2024-06-05 12:25:56,081 - root - INFO - Epoch 8, train loss: 2079.0113, val loss: 2321.6042
2024-06-05 12:25:57,746 - root - INFO - Epoch 9, train loss: 2078.2524, val loss: 2318.3161
2024-06-05 12:25:59,410 - root - INFO - Epoch 10, train loss: 2082.9989, val loss: 2317.6477
2024-06-05 12:26:01,220 - root - INFO - Epoch 11, train loss: 2077.3358, val loss: 2317.3104
2024-06-05 12:26:02,975 - root - INFO - Epoch 12, train loss: 2074.5096, val loss: 2319.1544
2024-06-05 12:26:04,716 - root - INFO - Epoch 13, train loss: 2076.0448, val loss: 2309.8287
2024-06-05 12:26:06,407 - root - INFO - Epoch 14, train loss: 2069.9774, val loss: 2309.7568
2024-06-05 12:26:08,286 - root - INFO - Epoch 15, train loss: 2068.9881, val loss: 2314.5475
2024-06-05 12:26:10,002 - root - INFO - Epoch 16, train loss: 2066.0724, val loss: 2311.8645
2024-06-05 12:26:11,755 - root - INFO - Epoch 17, train loss: 2064.6744, val loss: 2309.4448
2024-06-05 12:26:13,413 - root - INFO - Epoch 18, train loss: 2064.0907, val loss: 2314.9269
2024-06-05 12:26:15,132 - root - INFO - Epoch 19, train loss: 2061.6530, val loss: 2314.4241
2024-06-05 12:26:16,912 - root - INFO - Epoch 20, train loss: 2061.1232, val loss: 2307.8659
2024-06-05 12:26:18,569 - root - INFO - Epoch 21, train loss: 2060.9829, val loss: 2304.9851
2024-06-05 12:26:20,262 - root - INFO - Epoch 22, train loss: 2058.4910, val loss: 2305.4233
2024-06-05 12:26:22,039 - root - INFO - Epoch 23, train loss: 2059.6950, val loss: 2296.8896
2024-06-05 12:26:23,773 - root - INFO - Epoch 24, train loss: 2059.3666, val loss: 2303.2692
2024-06-05 12:26:25,477 - root - INFO - Epoch 25, train loss: 2057.0942, val loss: 2302.4821
2024-06-05 12:26:27,129 - root - INFO - Epoch 26, train loss: 2054.9840, val loss: 2305.6305
2024-06-05 12:26:28,766 - root - INFO - Epoch 27, train loss: 2053.3508, val loss: 2296.8391
2024-06-05 12:26:30,479 - root - INFO - Epoch 28, train loss: 2051.9990, val loss: 2301.3057
2024-06-05 12:26:32,144 - root - INFO - Epoch 29, train loss: 2049.9847, val loss: 2297.2571
2024-06-05 12:26:33,813 - root - INFO - Epoch 30, train loss: 2049.9069, val loss: 2304.2285
2024-06-05 12:26:35,536 - root - INFO - Epoch 31, train loss: 2047.6093, val loss: 2305.0038
2024-06-05 12:26:37,306 - root - INFO - Epoch 32, train loss: 2047.0760, val loss: 2307.0209
2024-06-05 12:26:39,014 - root - INFO - Epoch 33, train loss: 2045.7451, val loss: 2305.6578
2024-06-05 12:26:40,748 - root - INFO - Epoch 34, train loss: 2044.8883, val loss: 2301.3762
2024-06-05 12:26:42,418 - root - INFO - Epoch 35, train loss: 2043.9151, val loss: 2304.0649
2024-06-05 12:26:44,100 - root - INFO - Epoch 36, train loss: 2042.1132, val loss: 2304.2932
2024-06-05 12:26:45,752 - root - INFO - Epoch 37, train loss: 2041.8405, val loss: 2304.0518
2024-06-05 12:26:47,460 - root - INFO - Epoch 38, train loss: 2041.9502, val loss: 2302.6123
2024-06-05 12:26:49,115 - root - INFO - Epoch 39, train loss: 2041.5369, val loss: 2304.5182
2024-06-05 12:26:50,779 - root - INFO - Epoch 40, train loss: 2042.1694, val loss: 2302.7307
2024-06-05 12:26:52,596 - root - INFO - Epoch 41, train loss: 2040.8371, val loss: 2303.0438
2024-06-05 12:26:54,287 - root - INFO - Epoch 42, train loss: 2040.3228, val loss: 2302.0664
2024-06-05 12:26:56,027 - root - INFO - Epoch 43, train loss: 2040.0181, val loss: 2301.3938
2024-06-05 12:26:57,711 - root - INFO - Epoch 44, train loss: 2039.6256, val loss: 2301.6901
2024-06-05 12:26:59,481 - root - INFO - Epoch 45, train loss: 2039.4628, val loss: 2302.0378
2024-06-05 12:27:01,171 - root - INFO - Epoch 46, train loss: 2039.3035, val loss: 2302.0756
2024-06-05 12:27:02,897 - root - INFO - Epoch 47, train loss: 2039.1656, val loss: 2302.2287
2024-06-05 12:27:04,632 - root - INFO - Epoch 48, train loss: 2039.0822, val loss: 2302.1043
2024-06-05 12:27:06,317 - root - INFO - Epoch 49, train loss: 2039.0359, val loss: 2302.0815
2024-06-05 12:27:08,010 - root - INFO - Epoch 50, train loss: 2039.0140, val loss: 2302.0733
2024-06-05 12:27:08,011 - root - INFO - Optimizer: Adamax
2024-06-05 12:27:10,253 - root - INFO - Epoch 1, train loss: 2045.8564, val loss: 2306.2714
2024-06-05 12:27:12,389 - root - INFO - Epoch 2, train loss: 2044.1080, val loss: 2312.8131
2024-06-05 12:27:14,566 - root - INFO - Epoch 3, train loss: 2042.5492, val loss: 2311.9457
2024-06-05 12:27:16,785 - root - INFO - Epoch 4, train loss: 2039.8911, val loss: 2311.4402
2024-06-05 12:27:18,954 - root - INFO - Epoch 5, train loss: 2039.5476, val loss: 2310.7724
2024-06-05 12:27:21,228 - root - INFO - Epoch 6, train loss: 2037.2588, val loss: 2311.8480
2024-06-05 12:27:23,453 - root - INFO - Epoch 7, train loss: 2036.2523, val loss: 2316.6422
2024-06-05 12:27:25,681 - root - INFO - Epoch 8, train loss: 2035.8271, val loss: 2317.0468
2024-06-05 12:27:27,881 - root - INFO - Epoch 9, train loss: 2033.8035, val loss: 2320.8504
2024-06-05 12:27:30,061 - root - INFO - Epoch 10, train loss: 2031.5093, val loss: 2316.4633
2024-06-05 12:27:32,248 - root - INFO - Epoch 11, train loss: 2031.5490, val loss: 2319.5173
2024-06-05 12:27:34,396 - root - INFO - Epoch 12, train loss: 2029.1074, val loss: 2319.0525
2024-06-05 12:27:36,616 - root - INFO - Epoch 13, train loss: 2029.3183, val loss: 2322.6059
2024-06-05 12:27:38,851 - root - INFO - Epoch 14, train loss: 2028.7958, val loss: 2314.6209
2024-06-05 12:27:41,098 - root - INFO - Epoch 15, train loss: 2026.7063, val loss: 2318.1183
2024-06-05 12:27:43,290 - root - INFO - Epoch 16, train loss: 2029.5784, val loss: 2321.7333
2024-06-05 12:27:45,539 - root - INFO - Epoch 17, train loss: 2024.8593, val loss: 2319.9922
2024-06-05 12:27:47,720 - root - INFO - Epoch 18, train loss: 2027.2435, val loss: 2318.4910
2024-06-05 12:27:49,863 - root - INFO - Epoch 19, train loss: 2024.5612, val loss: 2315.2854
2024-06-05 12:27:52,071 - root - INFO - Epoch 20, train loss: 2025.9309, val loss: 2315.0347
2024-06-05 12:27:54,329 - root - INFO - Epoch 21, train loss: 2023.9233, val loss: 2319.1277
2024-06-05 12:27:56,565 - root - INFO - Epoch 22, train loss: 2024.5980, val loss: 2320.2264
2024-06-05 12:27:58,911 - root - INFO - Epoch 23, train loss: 2022.0110, val loss: 2320.3293
2024-06-05 12:28:01,096 - root - INFO - Epoch 24, train loss: 2022.3490, val loss: 2319.2614
2024-06-05 12:28:03,348 - root - INFO - Epoch 25, train loss: 2022.8892, val loss: 2321.0189
2024-06-05 12:28:05,562 - root - INFO - Epoch 26, train loss: 2021.2435, val loss: 2319.8235
2024-06-05 12:28:07,805 - root - INFO - Epoch 27, train loss: 2021.8251, val loss: 2319.6781
2024-06-05 12:28:09,992 - root - INFO - Epoch 28, train loss: 2021.2388, val loss: 2320.2923
2024-06-05 12:28:12,150 - root - INFO - Epoch 29, train loss: 2021.0432, val loss: 2318.4139
2024-06-05 12:28:14,420 - root - INFO - Epoch 30, train loss: 2018.8623, val loss: 2317.4724
2024-06-05 12:28:16,623 - root - INFO - Epoch 31, train loss: 2018.1801, val loss: 2317.4435
2024-06-05 12:28:18,823 - root - INFO - Epoch 32, train loss: 2017.2159, val loss: 2316.4819
2024-06-05 12:28:21,058 - root - INFO - Epoch 33, train loss: 2018.5933, val loss: 2313.6823
2024-06-05 12:28:23,281 - root - INFO - Epoch 34, train loss: 2017.0278, val loss: 2313.0088
2024-06-05 12:28:25,567 - root - INFO - Epoch 35, train loss: 2015.0422, val loss: 2310.0865
2024-06-05 12:28:27,759 - root - INFO - Epoch 36, train loss: 2015.0662, val loss: 2311.4295
2024-06-05 12:28:29,986 - root - INFO - Epoch 37, train loss: 2014.2960, val loss: 2314.1803
2024-06-05 12:28:32,170 - root - INFO - Epoch 38, train loss: 2014.2981, val loss: 2313.7100
2024-06-05 12:28:34,343 - root - INFO - Epoch 39, train loss: 2013.3552, val loss: 2312.4506
2024-06-05 12:28:36,550 - root - INFO - Epoch 40, train loss: 2012.7130, val loss: 2311.5917
2024-06-05 12:28:38,747 - root - INFO - Epoch 41, train loss: 2012.8019, val loss: 2310.8326
2024-06-05 12:28:40,980 - root - INFO - Epoch 42, train loss: 2012.7627, val loss: 2311.0259
2024-06-05 12:28:43,260 - root - INFO - Epoch 43, train loss: 2012.7848, val loss: 2310.7180
2024-06-05 12:28:45,544 - root - INFO - Epoch 44, train loss: 2012.3715, val loss: 2311.3914
2024-06-05 12:28:47,709 - root - INFO - Epoch 45, train loss: 2012.5458, val loss: 2310.9249
2024-06-05 12:28:49,867 - root - INFO - Epoch 46, train loss: 2012.4225, val loss: 2310.6219
2024-06-05 12:28:52,183 - root - INFO - Epoch 47, train loss: 2012.2497, val loss: 2310.6620
2024-06-05 12:28:54,369 - root - INFO - Epoch 48, train loss: 2012.1159, val loss: 2310.5783
2024-06-05 12:28:56,649 - root - INFO - Epoch 49, train loss: 2012.0290, val loss: 2310.5818
2024-06-05 12:28:58,817 - root - INFO - Epoch 50, train loss: 2012.0229, val loss: 2310.5834
2024-06-05 13:28:30,635 - root - INFO - Running on device=cuda
2024-06-05 13:28:31,053 - root - INFO - N_EPOCH: 100
2024-06-05 13:28:31,053 - root - INFO - LEARNING_RATE: 0.00075178
2024-06-05 13:28:31,053 - root - INFO - HIDDEN_LAYER: 24
2024-06-05 13:28:31,054 - root - INFO - HIDDEN_WIDTH: 64
2024-06-05 13:28:31,054 - root - INFO - EXPERIMENT_NAME: exp_call_12_gen
2024-06-05 13:28:31,054 - root - INFO - Model type: AmericanPut_gated3
2024-06-05 13:28:36,142 - root - INFO - Initial MSE train: 3281.511, Initial MSE val: 3561.4615
2024-06-05 13:28:39,518 - root - INFO - Total number of learnable parameters: 100365
2024-06-05 13:28:51,048 - root - INFO - N_EPOCH: 100
2024-06-05 13:28:51,049 - root - INFO - LEARNING_RATE: 0.00075178
2024-06-05 13:28:51,049 - root - INFO - HIDDEN_LAYER: 24
2024-06-05 13:28:51,049 - root - INFO - HIDDEN_WIDTH: 64
2024-06-05 13:28:51,050 - root - INFO - EXPERIMENT_NAME: exp_call_12_gen
2024-06-05 13:28:51,050 - root - INFO - Model type: AmericanPut_gated3
2024-06-05 13:28:54,109 - root - INFO - train set size: (8000, 9)
2024-06-05 13:28:54,109 - root - INFO - test set size: (2000, 9)
2024-06-05 13:28:54,110 - root - INFO - Initial MSE train: 3277.3386, Initial MSE val: 3578.1511
2024-06-05 13:28:54,873 - root - INFO - Total number of learnable parameters: 100365
2024-06-05 13:29:08,683 - root - INFO - Optimizer: SGD
2024-06-05 13:29:10,101 - root - INFO - Epoch 1, train loss: 1409515.8824, val loss: 25751.7462
2024-06-05 13:29:11,471 - root - INFO - Epoch 2, train loss: 16720.2164, val loss: 9438.1191
2024-06-05 13:29:12,875 - root - INFO - Epoch 3, train loss: 6464.2904, val loss: 5107.8716
2024-06-05 13:29:14,234 - root - INFO - Epoch 4, train loss: 4289.4034, val loss: 3892.3538
2024-06-05 13:29:15,633 - root - INFO - Epoch 5, train loss: 3161.0799, val loss: 2888.2350
2024-06-05 13:29:16,982 - root - INFO - Epoch 6, train loss: 2630.6990, val loss: 2799.7388
2024-06-05 13:29:18,360 - root - INFO - Epoch 7, train loss: 2263.5677, val loss: 2263.6558
2024-06-05 13:29:19,714 - root - INFO - Epoch 8, train loss: 2128.9936, val loss: 2451.0831
2024-06-05 13:29:21,110 - root - INFO - Epoch 9, train loss: 2008.6620, val loss: 2162.8321
2024-06-05 13:29:22,644 - root - INFO - Epoch 10, train loss: 2035.7888, val loss: 2318.9757
2024-06-05 13:29:24,156 - root - INFO - Epoch 11, train loss: 1980.3849, val loss: 2079.1475
2024-06-05 13:29:25,621 - root - INFO - Epoch 12, train loss: 1982.1958, val loss: 2448.1000
2024-06-05 13:29:27,200 - root - INFO - Epoch 13, train loss: 1963.8898, val loss: 3113.0598
2024-06-05 13:29:28,744 - root - INFO - Epoch 14, train loss: 1964.9993, val loss: 2150.8436
2024-06-05 13:29:30,335 - root - INFO - Epoch 15, train loss: 1937.8349, val loss: 2083.1809
2024-06-05 13:29:31,938 - root - INFO - Epoch 16, train loss: 1915.5456, val loss: 2345.0591
2024-06-05 13:29:33,399 - root - INFO - Epoch 17, train loss: 1992.8390, val loss: 2037.4383
2024-06-05 13:29:34,915 - root - INFO - Epoch 18, train loss: 1953.3407, val loss: 2109.8558
2024-06-05 13:29:36,329 - root - INFO - Epoch 19, train loss: 1984.5288, val loss: 2114.3108
2024-06-05 13:29:37,702 - root - INFO - Epoch 20, train loss: 1949.9809, val loss: 2189.4957
2024-06-05 13:29:39,097 - root - INFO - Epoch 21, train loss: 1910.0360, val loss: 2117.9761
2024-06-05 13:29:40,494 - root - INFO - Epoch 22, train loss: 1928.8188, val loss: 2836.3892
2024-06-05 13:29:42,034 - root - INFO - Epoch 23, train loss: 1928.0065, val loss: 2047.8182
2024-06-05 13:29:43,495 - root - INFO - Epoch 24, train loss: 1921.8426, val loss: 2146.9078
2024-06-05 13:29:44,947 - root - INFO - Epoch 25, train loss: 1958.7136, val loss: 2069.2910
2024-06-05 13:29:46,386 - root - INFO - Epoch 26, train loss: 1947.3159, val loss: 2078.0986
2024-06-05 13:29:47,796 - root - INFO - Epoch 27, train loss: 1892.0862, val loss: 2158.0144
2024-06-05 13:29:49,281 - root - INFO - Epoch 28, train loss: 1920.7856, val loss: 2023.0145
2024-06-05 13:29:50,933 - root - INFO - Epoch 29, train loss: 1916.9316, val loss: 2406.1305
2024-06-05 13:29:52,537 - root - INFO - Epoch 30, train loss: 1939.0821, val loss: 2111.3265
2024-06-05 13:29:54,327 - root - INFO - Epoch 31, train loss: 1925.0194, val loss: 2106.7423
2024-06-05 13:29:56,075 - root - INFO - Epoch 32, train loss: 1932.5633, val loss: 2140.4349
2024-06-05 13:29:57,850 - root - INFO - Epoch 33, train loss: 1903.9823, val loss: 2238.6566
2024-06-05 13:29:59,561 - root - INFO - Epoch 34, train loss: 1904.1260, val loss: 2179.7447
2024-06-05 13:30:01,249 - root - INFO - Epoch 35, train loss: 1903.2948, val loss: 2054.5735
2024-06-05 13:30:02,888 - root - INFO - Epoch 36, train loss: 1892.8738, val loss: 2117.8229
2024-06-05 13:30:04,499 - root - INFO - Epoch 37, train loss: 1942.5519, val loss: 2063.2678
2024-06-05 13:30:06,022 - root - INFO - Epoch 38, train loss: 1890.6335, val loss: 2357.0524
2024-06-05 13:30:07,596 - root - INFO - Epoch 39, train loss: 1915.4450, val loss: 2067.6192
2024-06-05 13:30:09,175 - root - INFO - Epoch 40, train loss: 1907.1569, val loss: 2101.6095
2024-06-05 13:30:10,660 - root - INFO - Epoch 41, train loss: 1879.4092, val loss: 2051.9784
2024-06-05 13:30:12,111 - root - INFO - Epoch 42, train loss: 1897.9960, val loss: 2226.0880
2024-06-05 13:30:13,520 - root - INFO - Epoch 43, train loss: 1890.5401, val loss: 2066.3301
2024-06-05 13:30:15,027 - root - INFO - Epoch 44, train loss: 1872.9741, val loss: 2060.0828
2024-06-05 13:30:16,634 - root - INFO - Epoch 45, train loss: 1870.4452, val loss: 2069.9919
2024-06-05 13:30:18,290 - root - INFO - Epoch 46, train loss: 1879.3437, val loss: 2084.0689
2024-06-05 13:30:20,039 - root - INFO - Epoch 47, train loss: 1871.6982, val loss: 2042.7820
2024-06-05 13:30:21,611 - root - INFO - Epoch 48, train loss: 1858.7693, val loss: 2036.3435
2024-06-05 13:30:23,065 - root - INFO - Epoch 49, train loss: 1868.8780, val loss: 2018.7224
2024-06-05 13:30:24,493 - root - INFO - Epoch 50, train loss: 1855.3128, val loss: 2026.6937
2024-06-05 13:30:25,903 - root - INFO - Epoch 51, train loss: 1851.7518, val loss: 2045.7554
2024-06-05 13:30:27,318 - root - INFO - Epoch 52, train loss: 1856.0738, val loss: 2106.1094
2024-06-05 13:30:28,960 - root - INFO - Epoch 53, train loss: 1869.7067, val loss: 2107.9540
2024-06-05 13:30:30,712 - root - INFO - Epoch 54, train loss: 1859.1114, val loss: 2153.1924
2024-06-05 13:30:32,398 - root - INFO - Epoch 55, train loss: 1845.9629, val loss: 2124.6461
2024-06-05 13:30:33,963 - root - INFO - Epoch 56, train loss: 1842.3852, val loss: 2025.0406
2024-06-05 13:30:35,376 - root - INFO - Epoch 57, train loss: 1857.1120, val loss: 2265.3524
2024-06-05 13:30:36,802 - root - INFO - Epoch 58, train loss: 1863.0773, val loss: 2030.7766
2024-06-05 13:30:38,375 - root - INFO - Epoch 59, train loss: 1837.4322, val loss: 2050.5494
2024-06-05 13:30:39,792 - root - INFO - Epoch 60, train loss: 1831.2943, val loss: 2032.2026
2024-06-05 13:30:41,193 - root - INFO - Epoch 61, train loss: 1837.1090, val loss: 2036.7474
2024-06-05 13:30:42,645 - root - INFO - Epoch 62, train loss: 1840.6126, val loss: 2031.1223
2024-06-05 13:30:44,138 - root - INFO - Epoch 63, train loss: 1830.9710, val loss: 2015.5377
2024-06-05 13:30:45,590 - root - INFO - Epoch 64, train loss: 1830.9527, val loss: 2024.6470
2024-06-05 13:30:47,113 - root - INFO - Epoch 65, train loss: 1822.6313, val loss: 2027.6837
2024-06-05 13:30:48,477 - root - INFO - Epoch 66, train loss: 1823.9525, val loss: 2007.9844
2024-06-05 13:30:49,850 - root - INFO - Epoch 67, train loss: 1821.3729, val loss: 2001.8943
2024-06-05 13:30:51,220 - root - INFO - Epoch 68, train loss: 1813.4331, val loss: 2010.6836
2024-06-05 13:30:52,589 - root - INFO - Epoch 69, train loss: 1818.8203, val loss: 2010.5220
2024-06-05 13:30:53,954 - root - INFO - Epoch 70, train loss: 1822.0300, val loss: 2012.7067
2024-06-05 13:30:55,364 - root - INFO - Epoch 71, train loss: 1820.2418, val loss: 2014.7703
2024-06-05 13:30:56,805 - root - INFO - Epoch 72, train loss: 1813.6434, val loss: 2008.8253
2024-06-05 13:30:58,275 - root - INFO - Epoch 73, train loss: 1811.3597, val loss: 2055.5529
2024-06-05 13:30:59,674 - root - INFO - Epoch 74, train loss: 1822.5576, val loss: 2039.8363
2024-06-05 13:31:01,097 - root - INFO - Epoch 75, train loss: 1816.1749, val loss: 2018.4687
2024-06-05 13:31:02,568 - root - INFO - Epoch 76, train loss: 1819.4664, val loss: 2037.0462
2024-06-05 13:31:04,124 - root - INFO - Epoch 77, train loss: 1818.8852, val loss: 2019.1149
2024-06-05 13:31:05,664 - root - INFO - Epoch 78, train loss: 1817.8105, val loss: 2028.5191
2024-06-05 13:31:07,139 - root - INFO - Epoch 79, train loss: 1816.1479, val loss: 2054.2438
2024-06-05 13:31:08,692 - root - INFO - Epoch 80, train loss: 1814.8586, val loss: 2041.3569
2024-06-05 13:31:10,157 - root - INFO - Epoch 81, train loss: 1808.0523, val loss: 2033.7813
2024-06-05 13:31:11,550 - root - INFO - Epoch 82, train loss: 1809.7503, val loss: 2033.4271
2024-06-05 13:31:13,106 - root - INFO - Epoch 83, train loss: 1809.4450, val loss: 2037.6114
2024-06-05 13:31:14,542 - root - INFO - Epoch 84, train loss: 1806.3903, val loss: 2033.4519
2024-06-05 13:31:15,992 - root - INFO - Epoch 85, train loss: 1806.0754, val loss: 2037.0359
2024-06-05 13:31:17,480 - root - INFO - Epoch 86, train loss: 1807.0196, val loss: 2035.4642
2024-06-05 13:31:18,896 - root - INFO - Epoch 87, train loss: 1805.8098, val loss: 2037.2542
2024-06-05 13:31:20,315 - root - INFO - Epoch 88, train loss: 1804.3657, val loss: 2045.5901
2024-06-05 13:31:21,738 - root - INFO - Epoch 89, train loss: 1805.2261, val loss: 2041.8127
2024-06-05 13:31:23,162 - root - INFO - Epoch 90, train loss: 1803.9557, val loss: 2035.1406
2024-06-05 13:31:24,565 - root - INFO - Epoch 91, train loss: 1803.0441, val loss: 2042.5102
2024-06-05 13:31:25,920 - root - INFO - Epoch 92, train loss: 1801.3598, val loss: 2042.2089
2024-06-05 13:31:27,310 - root - INFO - Epoch 93, train loss: 1801.8940, val loss: 2037.8663
2024-06-05 13:31:28,737 - root - INFO - Epoch 94, train loss: 1800.9869, val loss: 2035.3597
2024-06-05 13:31:30,119 - root - INFO - Epoch 95, train loss: 1801.3267, val loss: 2036.2962
2024-06-05 13:31:31,508 - root - INFO - Epoch 96, train loss: 1800.2022, val loss: 2039.3298
2024-06-05 13:31:32,911 - root - INFO - Epoch 97, train loss: 1799.8724, val loss: 2037.8758
2024-06-05 13:31:34,312 - root - INFO - Epoch 98, train loss: 1799.3887, val loss: 2038.3563
2024-06-05 13:31:35,735 - root - INFO - Epoch 99, train loss: 1799.1011, val loss: 2037.8190
2024-06-05 13:31:37,188 - root - INFO - Epoch 100, train loss: 1799.1227, val loss: 2037.7793
2024-06-05 13:33:41,572 - root - INFO - % of variation before training: 185.81
2024-06-05 13:33:41,573 - root - INFO - % of variation after training: 102.81
2024-06-05 13:33:41,573 - root - INFO - % of variation before training: 184.49
2024-06-05 13:33:41,574 - root - INFO - % of variation after training: 103.67
